{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sentiment analysis of online reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pylab as plt\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.cm as cm\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Download Sentiment Labelled Sentences Data Set. There are three data files under the root folder. yelp_labelled.txt, amazon_cells_labelled.txt and imdb_labelled.txt. Parse each file with the specifications in readme.txt. Are the labels balanced? If not, what’s the ratio between the two labels? Explain how you process these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 2)\n",
      "(1000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "#import pandas as pd\n",
    "yelp = pd.read_csv(\"yelp_labelled.txt\", header = None, delimiter = \"\\t\")\n",
    "amazon = pd.read_csv(\"amazon_cells_labelled.txt\", header = None, delimiter = '\\t')\n",
    "imdb = pd.read_csv(\"imdb_labelled.txt\", header = None, delimiter = '\\t+')\n",
    "\n",
    "print(yelp.shape)\n",
    "print(amazon.shape)\n",
    "print(imdb.shape)\n",
    "\n",
    "#print(yelp)\n",
    "#print(amazon)\n",
    "#print(imdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of 1 in yelp is  500\n",
      "The number of 0 in yelp is  500\n",
      "Thus, the labels in yelp are balenced\n"
     ]
    }
   ],
   "source": [
    "yelp_value = yelp.values\n",
    "yelp_1 = np.sum(yelp_value[:,1] == 1)\n",
    "print(\"The number of 1 in yelp is \", yelp_1)\n",
    "yelp_0 = np.sum(yelp_value[:,1] == 0)\n",
    "print(\"The number of 0 in yelp is \", yelp_0)\n",
    "\n",
    "if (yelp_1 == yelp_0):\n",
    "    print(\"Thus, the labels in yelp are balenced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of 1 in amazon is  500\n",
      "The number of 0 in amazon is  500\n",
      "Thus, the labels in amazon are balenced\n"
     ]
    }
   ],
   "source": [
    "amazon_value = amazon.values\n",
    "amazon_1 = np.sum(amazon_value[:,1] == 1)\n",
    "print(\"The number of 1 in amazon is \", amazon_1)\n",
    "amazon_0 = np.sum(amazon_value[:,1] == 0)\n",
    "print(\"The number of 0 in amazon is \", amazon_0)\n",
    "\n",
    "if (amazon_1 == amazon_0):\n",
    "    print(\"Thus, the labels in amazon are balenced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of 1 in imdb is  500\n",
      "The number of 0 in imdb is  500\n",
      "Thus, the labels in imdb are balenced\n"
     ]
    }
   ],
   "source": [
    "imdb_value = imdb.values\n",
    "imdb_1 = np.sum(imdb_value[:,1] == 1)\n",
    "print(\"The number of 1 in imdb is \", imdb_1)\n",
    "imdb_0 = np.sum(imdb_value[:,1] == 0)\n",
    "print(\"The number of 0 in imdb is \", imdb_0)\n",
    "\n",
    "if (imdb_1 == imdb_0):\n",
    "    print(\"Thus, the labels in imdb are balenced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pd.read_csv to read the data file and use numpy.sum to count the positive and negative amount of labels in each file. The labels are balanced since each of them has 500 positive and 500 negative in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Pick your preprocessing strategy. Since these sentences are online reviews, they may contain\n",
    "significant amounts of noise and garbage. You may or may not want to do one or all of\n",
    "the following. Explain the reasons for each of your decision (why or why not).\n",
    "• Lowercase all of the words.\n",
    "• Lemmatization of all the words (i.e., convert every word to its root so that all of “running,”\n",
    "“run,” and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best”\n",
    "are converted to “good”; this is easily done using nltk.stem).\n",
    "• Strip punctuation.\n",
    "• Strip the stop words, e.g., “the”, “and”, “or”.\n",
    "• Something else? Tell us about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    positive_data, positive_label = [], []\n",
    "    negative_data, negative_label = [], []\n",
    "    f = open(data, 'r')\n",
    "    for line in f:\n",
    "        temp = line.strip().split('\\t')\n",
    "        if temp[1] == '0':\n",
    "            positive_data.append(temp[0])\n",
    "            positive_label.append(int(temp[1]))\n",
    "        else: \n",
    "            negative_data.append(temp[0])\n",
    "            negative_label.append(int(temp[1]))\n",
    "    f.close()\n",
    "    return positive_data, positive_label, negative_data, negative_label\n",
    "\n",
    "amazon_positive_data, amazon_positive_label, amazon_negative_data, amazon_negative_label = read_data('amazon_cells_labelled.txt')\n",
    "imdb_positive_data, imdb_positive_label, imdb_negative_data, imdb_negative_label = read_data('imdb_labelled.txt')\n",
    "yelp_positive_data, yelp_positive_label, yelp_negative_data, yelp_negative_label = read_data('yelp_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data):\n",
    "    # Lowercase all of the words\n",
    "    data = np.char.lower(data)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #Strip puncturation\n",
    "    for i in range(len(data)):\n",
    "        data[i] = re.sub(r'[^\\w\\s]','',data[i])\n",
    "\n",
    "    #Lemmatization of all the words + Strip the stop words\n",
    "    for i in range(len(data)):\n",
    "        list_ = nltk.word_tokenize(data[i])\n",
    "        sen = []\n",
    "        for n in list_:\n",
    "            if not n in stop_words and not n.isdigit():\n",
    "                sen.append(lemmatizer.lemmatize(n))\n",
    "        data[i] = ' '.join(sen)\n",
    "    return data.tolist()\n",
    "\n",
    "#processing the data\n",
    "amazon_data_processed = preprocess_data(amazon_positive_data) + preprocess_data(amazon_negative_data)\n",
    "imdb_data_processed = preprocess_data(imdb_positive_data) + preprocess_data(imdb_negative_data)\n",
    "yelp_data_processed = preprocess_data(yelp_positive_data) + preprocess_data(yelp_negative_data)\n",
    "\n",
    "#double check the length of the data\n",
    "print(len(amazon_data_processed))\n",
    "print(len(yelp_data_processed))\n",
    "print(len(imdb_data_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Lowercase all of the words: Yes, it will help with key word matching.\n",
    "2.Lemmatization of all the words: Yes, it will help to increase the accuracy.\n",
    "3.Strip punctuation: Yes, punctuation is not taken into account.\n",
    "4.Strip the stop words: Yes, removing stop words can help extract more important information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Split training and testing set. In this assignment, for each file, please use the first 400 instances\n",
    "for each label as the training set and the remaining 100 instances as testing set. In\n",
    "total, there are 2400 reviews for training and 600 reviews for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "600\n",
      "2400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "#Split training and testing set\n",
    "#use the first 400 instances for each label as the training set and the remaining 100 instances as testing set\n",
    "\n",
    "#split training and testing set for yelp\n",
    "x_Train_yelp = yelp_data_processed[0:400] + yelp_data_processed[500:900]\n",
    "x_Test_yelp = yelp_data_processed[400:500] + yelp_data_processed[900:1000]\n",
    "y_Train_yelp = yelp_label_processed[0:400] + yelp_label_processed[500:900]\n",
    "y_Test_yelp = yelp_label_processed[400:500] + yelp_label_processed[900:1000]\n",
    "\n",
    "#split training and testing set for amazon\n",
    "x_Train_amazon = amazon_data_processed[0:400] + amazon_data_processed[500:900]\n",
    "x_Test_amazon = amazon_data_processed[400:500] + yelp_data_processed[900:1000]\n",
    "y_Train_amazon = amazon_label_processed[0:400] + amazon_label_processed[500:900]\n",
    "y_Test_amazon = amazon_label_processed[400:500] + yelp_label_processed[900:1000]\n",
    "\n",
    "#split training and testing set for imdb\n",
    "x_Train_imdb = imdb_data_processed[0:400] + imdb_data_processed[500:900]\n",
    "x_Test_imdb = imdb_data_processed[400:500] + yelp_data_processed[900:1000]\n",
    "y_Train_imdb = imdb_label_processed[0:400] + imdb_label_processed[500:900]\n",
    "y_Test_imdb = imdb_label_processed[400:500] + yelp_label_processed[900:1000]\n",
    "\n",
    "#In total, there are 2400 reviews for training and 600 reviews for testing\n",
    "x_train = x_Train_yelp + x_Train_amazon + x_Train_imdb\n",
    "x_test = x_Test_yelp + x_Test_amazon + x_Test_imdb\n",
    "y_train = y_Train_yelp + y_Train_amazon + y_Train_imdb\n",
    "y_test = y_Test_yelp + y_Test_amazon + y_Test_imdb\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "x_train = amazon_data_positive[:400] + amazon_data_negative[:400] +imdb_data_positive[:400]+imdb_data_negative[:400]+yelp_data_positive[:400]+yelp_data_negative[:400]\n",
    "x_test = amazon_data_positive[400:] + amazon_data_negative[400:] +imdb_data_positive[400:]+imdb_data_negative[400:]+yelp_data_positive[400:]+yelp_data_negative[400:]\n",
    "y_train = amazon_label_positive[:400] + amazon_label_negative[:400] +imdb_label_positive[:400]+imdb_label_negative[:400]+yelp_label_positive[:400]+yelp_label_negative[:400]\n",
    "y_test = amazon_label_positive[400:] + amazon_label_negative[400:] +imdb_label_positive[400:]+imdb_label_negative[400:]+yelp_label_positive[400:]+yelp_label_negative[400:]\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Bag of Words model. Extract features and then represent each review using bag of words\n",
    "model, i.e., every word in the review becomes its own element in a feature vector. In order to\n",
    "do this, first, make one pass through all the reviews in the training set (Explain why we can’t\n",
    "use testing set at this point) and build a dictionary of unique words. Then,make another pass\n",
    "through the review in both the training set and testing set and count up the occurrences of\n",
    "each word in your dictionary. The i th element of a review’s feature vector is the number of\n",
    "occurrences of the i th dictionary word in the review. Implement the bag of words model and\n",
    "report feature vectors of any two reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4194\n",
      "crust good\n",
      "2\n",
      "tasty texture nasty\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#bag of words model.\n",
    "dict_unique_words = dict()\n",
    "count_unique_word = 0\n",
    "for x in x_train:\n",
    "    for word in x.split(' '):\n",
    "        if word not in dict_unique_words:\n",
    "            dict_unique_words[word] = count_unique_word\n",
    "            count_unique_word +=1\n",
    "train_featured = [] \n",
    "test_featured = []  \n",
    "for x in x_train:\n",
    "    word_featured = [0]*count_unique_word\n",
    "    for word in x.split(' '):\n",
    "        word_featured[dict_unique_words[word]] += 1\n",
    "    train_featured.append(word_featured)\n",
    "\n",
    "for x in x_test:\n",
    "    word_featured = [0]*count_unique_word\n",
    "    for word in x.split(' '):  \n",
    "        if word in dict_unique_words:\n",
    "            word_featured[dict_unique_words[word]] += 1\n",
    "    test_featured.append(word_featured)\n",
    "\n",
    "train_featured = np.array(train_featured)\n",
    "test_featured = np.array(test_featured)\n",
    "\n",
    "\n",
    "print(count_unique_word)\n",
    "print(x_train[0])\n",
    "print(sum(train_featured[0]))\n",
    "print(x_train[1])\n",
    "print(sum(train_featured[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e)Pick your postprocessing strategy. Since the vast majority of English words will not appear in\n",
    "most of the reviews, most of the feature vector elements will be 0. This suggests that we need\n",
    "a postprocessing or normalization strategy that combats the huge variance of the elements\n",
    "in the feature vector. You may want to use one of the following strategies. Whatever choices\n",
    "you make, explain why you made the decision.\n",
    "• log-normalization. For each element of the feature vector x, transform it into f (x) Æ\n",
    "log (x Å1).\n",
    "• l1 normalization. Normalize the l1 normof the feature vector, ˆx Æ\n",
    "x\n",
    "j x j\n",
    ".\n",
    "• l2 normalization. Normalize the l2 normof the feature vector, ˆx Æ\n",
    "x\n",
    "kxk\n",
    ".\n",
    "• Standardize the data by subtracting the mean and dividing by the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: l2 can provide a higher accuracy of the normalization so we choose to postprocess the data through L2 strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69314718 0.69314718 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.69314718 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.69314718 0.         ... 0.69314718 0.69314718 0.69314718]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0.5        0.5        0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.33333333 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.0625     0.         ... 0.0625     0.0625     0.0625    ]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0.70710678 0.70710678 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.57735027 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.25       0.         ... 0.25       0.25       0.25      ]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[ 3.46265794e+01  3.33590038e+00 -6.13523873e-02 ... -2.04166684e-02\n",
      "  -2.04166684e-02 -2.04166684e-02]\n",
      " [-2.88795491e-02 -2.73730329e-01  1.62992842e+01 ... -2.04166684e-02\n",
      "  -2.04166684e-02 -2.04166684e-02]\n",
      " [-2.88795491e-02 -2.73730329e-01 -6.13523873e-02 ... -2.04166684e-02\n",
      "  -2.04166684e-02 -2.04166684e-02]\n",
      " ...\n",
      " [-2.88795491e-02 -2.73730329e-01 -6.13523873e-02 ... -2.04166684e-02\n",
      "  -2.04166684e-02 -2.04166684e-02]\n",
      " [-2.88795491e-02 -2.73730329e-01 -6.13523873e-02 ... -2.04166684e-02\n",
      "  -2.04166684e-02 -2.04166684e-02]\n",
      " [-2.88795491e-02  3.33590038e+00 -6.13523873e-02 ...  4.89795876e+01\n",
      "   4.89795876e+01  4.89795876e+01]]\n",
      "[[ 0.         -0.26231865 -0.10050378 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -0.26231865 -0.10050378 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -0.26231865 -0.10050378 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.         -0.26231865 -0.10050378 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -0.26231865 -0.10050378 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -0.26231865 -0.10050378 ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#postprocessing\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#log-normalization. For each element of the feature vector x, transform it into f (x) Æ log (x Å1)\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "train_featured_log = transformer.transform(train_featured)\n",
    "test_featured_log = transformer.transform(test_featured)\n",
    "print(train_featured_log)\n",
    "print(test_featured_log)\n",
    "\n",
    "#I1 normalization. Normalize the l1 normof the feature vector, ˆx Æ x j x j \n",
    "train_featured_l1 = preprocessing.normalize(train_featured, norm='l1')\n",
    "test_featured_l1 = preprocessing.normalize(test_featured, norm='l1')\n",
    "print(train_featured_l1)\n",
    "print(test_featured_l1)\n",
    "\n",
    "#l2 normalization. Normalize the l2 normof the feature vector, ˆx Æ x kxk \n",
    "train_featured_l2 = preprocessing.normalize(train_featured, norm='l2')\n",
    "test_featured_l2 = preprocessing.normalize(test_featured, norm='l2')\n",
    "print(train_featured_l2)\n",
    "print(test_featured_l2)\n",
    "\n",
    "#Standardize the data by subtracting the mean and dividing by the variance.\n",
    "train_featured_scale = preprocessing.scale(train_featured)\n",
    "test_featured_scale = preprocessing.scale(test_featured)\n",
    "print(train_featured_scale)\n",
    "print(test_featured_scale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f ) Sentiment prediction. Train a logistic regression model (you can use existing packages here)\n",
    "on the training set and test on the testing set. Report the classification accuracy and confusion\n",
    "matrix. Inspecting the weight vector of the logistic regression, what are the words that\n",
    "play the most important roles in deciding the sentiment of the reviews? Repeat this with a\n",
    "Naive Bayes classifier and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Accuracy: 0.8066666666666666\n",
      "Confusion matrix for logistic regression:\n",
      " [[253  47]\n",
      " [ 69 231]]\n",
      "Bernoulli Accuracy: 0.8133333333333334\n",
      "Confusion matrix for Naive Bayes:\n",
      " [[251  49]\n",
      " [ 63 237]]\n",
      "Top 10 important word of logistic: ['good', 'worst', 'poor', 'bad', 'love', 'best', 'nice', 'great', 'delicious', 'excellent']\n",
      "Top 10 important word of Naive Bayes: ['crust', 'chilly', 'unremarkable', 'author', 'livingworking', 'abstruse', 'culture', 'reenactment', 'emotionally', 'masculinity']\n"
     ]
    }
   ],
   "source": [
    "#Sentiment prediction\n",
    "\n",
    "#classfication accuracy\n",
    "#Logistic Regression\n",
    "L_R = LogisticRegression()\n",
    "L_R.fit(train_featured_l2,y_train)\n",
    "y_prediction_LR = L_R.predict(test_featured_l2)\n",
    "print(\"Logistic Accuracy:\",metrics.accuracy_score(y_test, y_prediction_LR))\n",
    "#Confusion Matrix for Logistic Regression\n",
    "cm_LR = confusion_matrix(y_test, y_prediction_LR)\n",
    "print(\"Confusion matrix for logistic regression:\\n\", cm_LR)\n",
    "\n",
    "#Naive Bayes classfier\n",
    "Ber_NB = BernoulliNB()\n",
    "Ber_NB.fit(train_featured_l2,y_train)\n",
    "y_prediction_NB = Ber_NB.predict(test_featured_l2)\n",
    "print(\"Bernoulli Accuracy:\",metrics.accuracy_score(y_test, y_prediction_NB))\n",
    "#Confusion Matrix for Naive Bayes\n",
    "cm_NB = confusion_matrix(y_test, y_prediction_NB)\n",
    "print(\"Confusion matrix for Naive Bayes:\\n\", cm_NB)\n",
    "\n",
    "# the words that play the most important roles in deciding the sentiment of the reviews\n",
    "def Most_Important_Word(m, d):\n",
    "    freq = abs(m.coef_[0])\n",
    "    most_important_word_idx = np.argsort(freq)[::-1][:10]\n",
    "    re = []\n",
    "    for word,value in d.items():\n",
    "        if value in most_important_word_idx: \n",
    "            re.append(str(word))\n",
    "    return re\n",
    "print(\"Top 10 important word of Logistic Regression:\", Most_Important_Word(L_R, dict_unique_words))\n",
    "print(\"Top 10 important word of Naive Bayes:\", Most_Important_Word(Ber_NB, dict_unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) N-gram model. Similar to the bag of words model, but now you build up a dictionary of ngrams,\n",
    "which are contiguous sequences of words. For example, “Alice fell down the rabbit\n",
    "hole” would then map to the 2-grams sequence: [\"Alice fell\", \"fell down\", \"down the\", \"the\n",
    "rabbit\", \"rabbit hole\"], and all five of those symbols would be members of the n-gram dictionary.\n",
    "Try n Æ 2, repeat (d)-(g) and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Accuracy: 0.64\n",
      "Top 10 important word of Logistic Regression: ['is not', 'was not', 'waste your', 'and the', 'was very', 'would not', 'the worst', 'the best', 'a great', 'I love']\n",
      "Confusion matrix for Logistic Regression:\n",
      " [[229  71]\n",
      " [145 155]]\n"
     ]
    }
   ],
   "source": [
    "#N-gram model\n",
    "\n",
    "n_gram_unique_word_dict = dict()\n",
    "n_gram_unique_word_count = 0\n",
    "n_gram_train_featured = [] \n",
    "n_gram_test_featured = []  \n",
    "\n",
    "for n in x_train:\n",
    "    words = n.split()\n",
    "    for i in range(len(words)-2):\n",
    "        sequence = ' '.join(words[i:i+2])\n",
    "        if sequence not in n_gram_unique_word_dict:\n",
    "            n_gram_unique_word_dict[sequence] = unique_word_count\n",
    "            unique_word_count +=1\n",
    "\n",
    "for n in x_train:\n",
    "    word_featured = [0]*unique_word_count\n",
    "    words = n.split()\n",
    "    for i in range(len(words) - 2):\n",
    "        sequence = ' '.join(words[i:i + 2])\n",
    "        word_featured[n_gram_unique_word_dict[sequence]] += 1\n",
    "    n_gram_train_featured.append(word_featured)\n",
    "n_gram_train_featured_l2 = preprocessing.normalize(n_gram_train_featured, norm='l2')\n",
    "\n",
    "for n in x_test:\n",
    "    word_featured = [0]*unique_word_count\n",
    "    words = n.split()\n",
    "    for i in range(len(words) - 2):\n",
    "        sequence = ' '.join(words[i:i + 2])\n",
    "        if sequence in n_gram_unique_word_dict:\n",
    "            word_featured[n_gram_unique_word_dict[sequence]] += 1\n",
    "    n_gram_test_featured.append(word_featured)\n",
    "    \n",
    "n_gram_test_featured_l2 = preprocessing.normalize(n_gram_test_featured, norm='l2')\n",
    "\n",
    "L_R = LogisticRegression()\n",
    "L_R.fit(n_gram_train_featured_l2,y_train)\n",
    "y_prediction_LR = L_R.predict(n_gram_test_featured_l2)\n",
    "print(\"Logistic Accuracy:\",metrics.accuracy_score(y_test, y_prediction_LR))\n",
    "print(\"Top 10 important word of Logistic Regression:\", find_important(L_R, n_gram_unique_word_dict))\n",
    "cm_lr = confusion_matrix(y_test, y_prediction_LR)\n",
    "print(\"Confusion matrix for Logistic Regression:\\n\", cm_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) PCA for bag of words model. The features in the bag of words model have large redundancy.\n",
    "Implement PCA to reduce the dimension of features calculated in (e) to 10, 50 and 100 respectively.\n",
    "Using these lower-dimensional feature vectors and repeat (f ), (g). Report corresponding\n",
    "clustering and classification results. (Note: You should implement PCA yourself,\n",
    "but you can use numpy.svd or some other SVD package. Feel free to double-check your PCA\n",
    "implementation against an existing one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA for bag of words:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of features: 10, accuracy after PCA: 0.35333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of features: 50, accuracy after PCA: 0.3466666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of features: 100, accuracy after PCA: 0.395\n",
      "2-gram:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of features: 10, accuracy after PCA: 0.485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of features: 50, accuracy after PCA: 0.465\n",
      "dimension of features: 100, accuracy after PCA: 0.46166666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#PCA for bag of words model\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "#Implement PCA\n",
    "def PCA(features,n):\n",
    "    t = np.mean(features, 0) # mean of each column\n",
    "    features = features - t\n",
    "    eva, evs, eve = np.linalg.svd(features, full_matrices=False)\n",
    "    e_ = eva[:, :n]*evs[:n]\n",
    "    return e_\n",
    "\n",
    "def lg(pca_x_train, pca_x_test, y_train, y_test):\n",
    "    L_R = LogisticRegression()\n",
    "    L_R.fit(pca_x_train,y_train)\n",
    "    y_prediction_LR = L_R.predict(pca_x_test)\n",
    "    return metrics.accuracy_score(y_test, y_prediction_LR)\n",
    "\n",
    "def Implement_PCA(train_featured,test_featured, n):  \n",
    "    pca_x_train = PCA(train_featured,n)\n",
    "    pca_x_test = PCA(test_featured,n)\n",
    "    accuracy = lg(pca_x_train, pca_x_test, y_train, y_test)\n",
    "    print(\"dimension of features:\", n, \"accuracy after PCA:\", accuracy)\n",
    "\n",
    "#to reduce the dimension of features calculated in (e) to 10, 50 and 100 respectively.\n",
    "print(\"PCA for bag of words:\")\n",
    "Implement_PCA(train_featured, test_featured, 10)\n",
    "Implement_PCA(train_featured, test_featured, 50)\n",
    "Implement_PCA(train_featured, test_featured, 100)\n",
    "print(\"2-gram:\")\n",
    "Implement_PCA(n_gram_train_featured, n_gram_test_featured, 10)\n",
    "Implement_PCA(n_gram_train_featured, n_gram_test_featured, 50)\n",
    "Implement_PCA(n_gram_train_featured, n_gram_test_featured, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the result, when dimensions of features is 100, after pca, its accuracy is the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.42417182e-02  8.65271223e-01  1.44121633e-01 ... -1.35898162e-02\n",
      "  -1.26040918e-02 -1.04450488e-02]\n",
      " [ 1.03991568e-02 -4.80540250e-03 -1.27947749e-01 ...  3.85017940e-02\n",
      "   3.69153521e-02 -2.14583840e-02]\n",
      " [ 6.70847491e-05 -6.67484864e-03 -1.35120194e-01 ... -1.17845867e-01\n",
      "   9.49185506e-02  2.66480539e-02]\n",
      " ...\n",
      " [-4.55276367e-02  2.96946723e-03 -8.80614976e-02 ... -1.31945176e-01\n",
      "  -7.07224558e-02 -7.24686186e-02]\n",
      " [-1.24948553e-01 -2.58973259e-02 -4.21827373e-02 ... -1.78451911e-02\n",
      "   5.11616847e-02 -1.09037247e-01]\n",
      " [ 6.47968718e-02  9.24611277e-01  1.96114322e-01 ... -7.74640416e-02\n",
      "  -2.05394148e-01  1.13887436e-01]]\n",
      "Logistic Accuracy: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#solve the logistic regression for dimensions of features = 100\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 100)\n",
    "pca_1 = pca.fit_transform(train_featured)\n",
    "pca_2 = pca.fit_transform(test_featured)\n",
    "L_R = LogisticRegression()\n",
    "L_R.fit(pca_1,y_train)\n",
    "y_prediction_LR_100 = L_R.predict(pca_2)\n",
    "print(\"Logistic Accuracy:\",metrics.accuracy_score(y_test, y_prediction_LR_100))\n",
    "print(pca_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Algorithms comparison and analysis. According to the above results, compare the performances\n",
    "of bag of words, 2-gram and PCA for bag of words. Which method performs best in\n",
    "the prediction task and why? What do you learn about the language that people use in online\n",
    "reviews (e.g., expressions that will make the posts positive/negative)? Hint: Inspect the\n",
    "clustering results and the weights learned from logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can find that the bag of words method through Logistic model performs best in the prediction task. It might be becasue it will not reduce dimensions of features and preserve meaningful information such as words and sentences itself.\n",
    "From the results of the top 10, we can find that most important words are positive emotional expressions such as \"good\", \"delicious\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
